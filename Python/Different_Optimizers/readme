Optimization Algorithms Comparison on California Housing DatasetThis project performs a comparative analysis of three different optimization algorithmsâ€”L-BFGS, Stochastic Gradient Descent (SGD), and Manual Gradient Descentâ€”applied to a Linear Regression model. The goal is to predict housing prices using the California Housing dataset after performing statistical data cleaning and feature engineering.ðŸ“„ Project OverviewThe workflow of the project is as follows:Data Loading: Importing the California Housing dataset.Preprocessing: Statistical outlier removal using Z-Scores and data standardization.Feature Selection: Using Mutual Information to select relevant features.Model Training: Training a Linear Regression model using three different optimization strategies.Evaluation: Comparing Mean Squared Error (MSE) and convergence speeds.ðŸ“Š Dataset & PreprocessingThe project uses the California Housing dataset loaded via sklearn.Outlier Removal (Z-Score)To ensure the model is robust, outliers were removed using statistical analysis:Z-Score Calculation: The Z-score was computed for numerical columns to measure how many standard deviations a data point is from the mean.Thresholding: A threshold of 1.5 was applied. Any row containing a value with a Z-score greater than 1.5 was considered an outlier and dropped from the dataset.Feature EngineeringMutual Information: mutual_info_regression was used to determine feature importance.Feature Dropping: Based on the analysis, the "AveRooms" and "Population" columns were dropped to reduce noise.Scaling: The data was standardized using StandardScaler to ensure efficient convergence for the optimization algorithms.ðŸ§  Optimization Algorithms Implemented1. L-BFGS (Limited-memory BFGS)Type: Quasi-Newton Method (Second-order optimization).Implementation: PyTorch optim.LBFGS.Performance: This optimizer demonstrated extremely fast convergence.Iterations: Converged significantly within 10-20 iterations.Test MSE: Approximately 0.3667.2. Stochastic Gradient Descent (SGD)Type: First-order optimization.Implementation: PyTorch optim.SGD with a learning rate of 0.1.Performance: Required more iterations to converge compared to L-BFGS.Iterations: Run for 41 iterations.Test MSE: Approximately 0.3673.3. Manual Gradient DescentType: First-order optimization (implemented from scratch using NumPy).Implementation: Calculated gradients manually using matrix multiplication:$\frac{\partial J}{\partial w} = \frac{2}{N} X^T (Xw + b - y)$$\frac{\partial J}{\partial b} = \frac{2}{N} \sum (Xw + b - y)$Performance:Epochs: 401 epochs.Final Loss: 0.3505.
