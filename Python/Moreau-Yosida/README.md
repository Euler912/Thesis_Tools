Moreau-Yosida Regularization
This repository contains code and tools related to Moreau-Yosida regularization, a technique used in optimization and analysis of non-smooth functions.
ğŸ“š What is Moreau-Yosida Regularization?
Moreau-Yosida regularization smooths a non-smooth (possibly non-differentiable) function.
Given a function $g : \mathbb{R}^n \to \mathbb{R} \cup {+\infty}$ and a parameter $\lambda > 0$, the regularized function $g_\lambda$ is defined as:
gÎ»(x)=minâ¡yâˆˆRn{g(y)+12Î»âˆ¥yâˆ’xâˆ¥2}g_\lambda(x) = \min_{y \in \mathbb{R}^n} \{ g(y) + \frac{1}{2\lambda} \|y - x\|^2 \}gÎ»â€‹(x)=minyâˆˆRnâ€‹{g(y)+2Î»1â€‹âˆ¥yâˆ’xâˆ¥2}

Small $\lambda$: Closer approximation to $g(x)$.
Large $\lambda$: Smoother version of $g(x)$.

The point that minimizes the above expression is called the proximal operator:
proxÎ»g(x)=argâ¡minâ¡y{g(y)+12Î»âˆ¥yâˆ’xâˆ¥2}\text{prox}_{\lambda g}(x) = \arg\min_{y} \{ g(y) + \frac{1}{2\lambda} \|y - x\|^2 \}proxÎ»gâ€‹(x)=argminyâ€‹{g(y)+2Î»1â€‹âˆ¥yâˆ’xâˆ¥2}
The gradient of $g_\lambda$ is given by:
âˆ‡gÎ»(x)=1Î»(xâˆ’proxÎ»g(x))\nabla g_\lambda(x) = \frac{1}{\lambda}(x - \text{prox}_{\lambda g}(x))âˆ‡gÎ»â€‹(x)=Î»1â€‹(xâˆ’proxÎ»gâ€‹(x))
âœ¨ Properties

$g_\lambda(x)$ is convex if $g(x)$ is convex.
$g_\lambda(x)$ is differentiable (even if $g$ is not).
The gradient $\nabla g_\lambda(x)$ is Lipschitz continuous with constant $1/\lambda$.
As $\lambda \to 0$, $g_\lambda(x) \to g(x)$ (pointwise).

ğŸ¯ Why Use It?

To smooth non-differentiable functions.
To enable gradient-based optimization methods.
To analyze non-smooth problems with smoother approximations.
To connect optimization and proximal algorithms.
